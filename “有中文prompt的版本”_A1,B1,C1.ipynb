{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XiaoyingHeAarhus/Interact-LLM/blob/main/%E2%80%9C%E6%9C%89%E4%B8%AD%E6%96%87prompt%E7%9A%84%E7%89%88%E6%9C%AC%E2%80%9D_A1%2CB1%2CC1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW_nvUnxDFTC",
        "outputId": "1a3fea6d-a93f-4791-80ab-9d9456411b50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Interact-LLM'...\n",
            "remote: Enumerating objects: 636, done.\u001b[K\n",
            "remote: Counting objects: 100% (212/212), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 636 (delta 126), reused 154 (delta 100), pack-reused 424 (from 1)\u001b[K\n",
            "Receiving objects: 100% (636/636), 177.25 KiB | 1.48 MiB/s, done.\n",
            "Resolving deltas: 100% (295/295), done.\n",
            "/content/Interact-LLM\n",
            "Collecting uv\n",
            "  Downloading uv-0.9.27-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.9.27-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.7/22.7 MB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uv\n",
            "Successfully installed uv-0.9.27\n",
            "\u001b[2mInstalled \u001b[1mPython 3.12.12\u001b[0m \u001b[2min 2.58s\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcpython-3.12.12-linux-x86_64-gnu\u001b[0m (python3.12)\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`\u001b[36m/root/.local/bin\u001b[39m` is not on your PATH. To use installed Python executables, run `\u001b[32mexport PATH=\"/root/.local/bin:$PATH\"\u001b[39m` or `\u001b[32muv python update-shell\u001b[39m`.\u001b[0m\n",
            "Using CPython \u001b[36m3.12.12\u001b[39m\u001b[36m\u001b[39m\n",
            "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
            "\u001b[2mResolved \u001b[1m63 packages\u001b[0m \u001b[2min 0.93ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m62 packages\u001b[0m \u001b[2min 1m 25s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m62 packages\u001b[0m \u001b[2min 493ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.5.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.1.31\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.18.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.29.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1minteract-llm\u001b[0m\u001b[2m==0.1.0 (from file:///content/Interact-LLM)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlingua-language-detector\u001b[0m\u001b[2m==2.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlinkify-it-py\u001b[0m\u001b[2m==2.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmdit-py-plugins\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmlx\u001b[0m\u001b[2m==0.23.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmlx-lm\u001b[0m\u001b[2m==0.21.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.2.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.21.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==24.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.3.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.30.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.10.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.27.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2024.11.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==13.9.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mruff\u001b[0m\u001b[2m==0.11.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.5.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msentencepiece\u001b[0m\u001b[2m==0.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==76.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtextual\u001b[0m\u001b[2m==2.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtoml\u001b[0m\u001b[2m==0.10.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.50.0.dev0 (from git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.12.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1muc-micro-py\u001b[0m\u001b[2m==1.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.3.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# 1. ÂÖãÈöÜ‰ª£Á†Å\n",
        "!git clone https://github.com/XiaoyingHeAarhus/Interact-LLM.git\n",
        "%cd Interact-LLM\n",
        "\n",
        "# 2. ÂÆâË£Ö uv Âíå‰æùËµñ\n",
        "!pip install uv\n",
        "# Âº∫Âà∂‰ΩøÁî® Python 3.12 Âπ∂Âà†ÊéâÊ≤°Áî®ÁöÑ mlx (Colab ÈªòËÆ§ÊòØ 3.11+)\n",
        "!uv python install 3.12\n",
        "!uv sync --python 3.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Interact-LLM\n",
        "\n",
        "for i in range(1, 31):\n",
        "    print(f\"üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ {i} ËΩÆÂÆûÈ™å...\")\n",
        "    # Ê≥®ÊÑèËøôÈáå‰ΩøÁî®ÁöÑÊòØ name ÂÆö‰πâÁöÑÁÆÄÁß∞ qwen25_15b\n",
        "    !NO_COLOR=1 uv run python src/scripts/alignment_drift/simulate.py \\\n",
        "        --model_name \"qwen25_15b\" \\\n",
        "        --prompt_version \"999\" \\\n",
        "        --backend hf\n",
        "    print(f\"‚úÖ Á¨¨ {i} ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\")\n",
        "\n",
        "print(\"üéä ÊÅ≠ÂñúÔºÅ30 ËΩÆ‰∏≠ÊñáÂØπËØùÊ®°ÊãüÂÖ®ÈÉ®ËøêË°åÂÆåÊØïÔºÅ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ik6G47gA2GS",
        "outputId": "463a5262-59be-41d5-eb8a-433c8ada092c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Interact-LLM\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 1 ËΩÆÂÆûÈ™å...\n",
            "[INFO]: Running simulation run 1 out of 30\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:17<00:00,  8.63s/it]\n",
            "[INFO]: Running simulation run 2 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:01<00:00,  6.84s/it]\n",
            "[INFO]: Running simulation run 3 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:33<00:00,  3.77s/it]\n",
            "[INFO]: Running simulation run 4 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:34<00:00,  3.83s/it]\n",
            "[INFO]: Running simulation run 5 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:16<00:00,  1.87s/it]\n",
            "[INFO]: Running simulation run 6 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:07<00:00,  7.46s/it]\n",
            "[INFO]: Running simulation run 7 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:29<00:00,  9.89s/it]\n",
            "[INFO]: Running simulation run 8 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:40<00:00, 11.16s/it]\n",
            "[INFO]: Running simulation run 9 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:39<00:00,  4.42s/it]\n",
            "[INFO]: Running simulation run 10 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:48<00:00, 12.05s/it]\n",
            "[INFO]: Running simulation run 11 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:13<00:00,  8.16s/it]\n",
            "[INFO]: Running simulation run 12 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [02:42<00:00, 18.04s/it]\n",
            "[INFO]: Running simulation run 13 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:50<00:00,  5.57s/it]\n",
            "[INFO]: Running simulation run 14 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:58<00:00,  6.47s/it]\n",
            "[INFO]: Running simulation run 15 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:09<00:00,  7.72s/it]\n",
            "[INFO]: Running simulation run 16 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:43<00:00,  4.87s/it]\n",
            "[INFO]: Running simulation run 17 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:29<00:00,  9.92s/it]\n",
            "[INFO]: Running simulation run 18 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:44<00:00,  4.98s/it]\n",
            "[INFO]: Running simulation run 19 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:35<00:00,  3.95s/it]\n",
            "[INFO]: Running simulation run 20 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:09<00:00,  7.75s/it]\n",
            "[INFO]: Running simulation run 21 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [04:18<00:00, 28.73s/it]\n",
            "[INFO]: Running simulation run 22 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:52<00:00,  5.89s/it]\n",
            "[INFO]: Running simulation run 23 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:37<00:00,  4.21s/it]\n",
            "[INFO]: Running simulation run 24 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:43<00:00,  4.87s/it]\n",
            "[INFO]: Running simulation run 25 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:51<00:00, 12.35s/it]\n",
            "[INFO]: Running simulation run 26 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:35<00:00,  3.90s/it]\n",
            "[INFO]: Running simulation run 27 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:23<00:00,  9.31s/it]\n",
            "[INFO]: Running simulation run 28 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:45<00:00,  5.03s/it]\n",
            "[INFO]: Running simulation run 29 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:33<00:00,  3.72s/it]\n",
            "[INFO]: Running simulation run 30 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:54<00:00,  6.00s/it]\n",
            "‚úÖ Á¨¨ 1 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 2 ËΩÆÂÆûÈ™å...\n",
            "[INFO]: Running simulation run 1 out of 30\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:42<00:00,  4.70s/it]\n",
            "[INFO]: Running simulation run 2 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:22<00:00,  2.52s/it]\n",
            "[INFO]: Running simulation run 3 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:09<00:00,  7.67s/it]\n",
            "[INFO]: Running simulation run 4 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:41<00:00,  4.64s/it]\n",
            "[INFO]: Running simulation run 5 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:51<00:00,  5.74s/it]\n",
            "[INFO]: Running simulation run 6 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:43<00:00, 11.49s/it]\n",
            "[INFO]: Running simulation run 7 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:49<00:00,  5.53s/it]\n",
            "[INFO]: Running simulation run 8 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:44<00:00, 11.62s/it]\n",
            "[INFO]: Running simulation run 9 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [02:29<00:00, 16.64s/it]\n",
            "[INFO]: Running simulation run 10 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:45<00:00,  5.07s/it]\n",
            "[INFO]: Running simulation run 11 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [00:43<00:00,  4.88s/it]\n",
            "[INFO]: Running simulation run 12 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            " 44% 4/9 [00:27<00:33,  6.80s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 211, in <module>\n",
            "    main()\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 177, in main\n",
            "    tutor_history = simulate_conversation(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 100, in simulate_conversation\n",
            "    tutor_message = model.generate(tutor_history)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 92, in generate\n",
            "    output = self.model.generate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2250, in generate\n",
            "    result = self._sample(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 3227, in _sample\n",
            "    while self._has_unfinished_sequences(\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 2 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 3 ËΩÆÂÆûÈ™å...\n",
            "[INFO]: Running simulation run 1 out of 30\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "100% 9/9 [01:00<00:00,  6.73s/it]\n",
            "[INFO]: Running simulation run 2 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            " 11% 1/9 [00:11<01:33, 11.63s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 211, in <module>\n",
            "    main()\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 177, in main\n",
            "    tutor_history = simulate_conversation(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 117, in simulate_conversation\n",
            "    student_message = model.generate(student_history)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 92, in generate\n",
            "    output = self.model.generate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2250, in generate\n",
            "    result = self._sample(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 3241, in _sample\n",
            "    outputs = model_forward(**model_inputs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 855, in forward\n",
            "    outputs = self.model(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 579, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 260, in forward\n",
            "    hidden_states, self_attn_weights = self.self_attn(\n",
            "                                       ^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 167, in forward\n",
            "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 91, in apply_rotary_pos_emb\n",
            "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
            "                           ^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 65, in rotate_half\n",
            "    return torch.cat((-x2, x1), dim=-1)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 3 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 4 ËΩÆÂÆûÈ™å...\n",
            "[INFO]: Running simulation run 1 out of 30\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            " 11% 1/9 [00:05<00:47,  6.00s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 211, in <module>\n",
            "    main()\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 177, in main\n",
            "    tutor_history = simulate_conversation(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 117, in simulate_conversation\n",
            "    student_message = model.generate(student_history)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 92, in generate\n",
            "    output = self.model.generate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2250, in generate\n",
            "    result = self._sample(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 3241, in _sample\n",
            "    outputs = model_forward(**model_inputs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 855, in forward\n",
            "    outputs = self.model(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 579, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 276, in forward\n",
            "    hidden_states = self.mlp(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 57, in forward\n",
            "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "                                           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 4 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 5 ËΩÆÂÆûÈ™å...\n",
            "[INFO]: Running simulation run 1 out of 30\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            "  0% 0/9 [00:01<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 211, in <module>\n",
            "    main()\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 177, in main\n",
            "    tutor_history = simulate_conversation(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 100, in simulate_conversation\n",
            "    tutor_message = model.generate(tutor_history)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 92, in generate\n",
            "    output = self.model.generate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2250, in generate\n",
            "    result = self._sample(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 3241, in _sample\n",
            "    outputs = model_forward(**model_inputs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 855, in forward\n",
            "    outputs = self.model(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 579, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 260, in forward\n",
            "    hidden_states, self_attn_weights = self.self_attn(\n",
            "                                       ^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 163, in forward\n",
            "    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 5 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 6 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 405, in <module>\n",
            "    from torch._C import *  # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 463, in _lock_unlock_module\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 6 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 7 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1873, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1885, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/__init__.py\", line 15, in <module>\n",
            "    from . import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/dinov2_with_registers/__init__.py\", line 27, in <module>\n",
            "    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)\n",
            "                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 2309, in define_import_structure\n",
            "    import_structure = create_import_structure_from_path(module_path)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 2178, in create_import_structure_from_path\n",
            "    for _all_object in fetch__all__(file_content):\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1958, in fetch__all__\n",
            "    lines = file_content.splitlines()\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 7 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 8 ËΩÆÂÆûÈ™å...\n",
            "[INFO]: Running simulation run 1 out of 30\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            " 78% 7/9 [00:20<00:05,  2.93s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 211, in <module>\n",
            "    main()\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 177, in main\n",
            "    tutor_history = simulate_conversation(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 100, in simulate_conversation\n",
            "    tutor_message = model.generate(tutor_history)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 92, in generate\n",
            "    output = self.model.generate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2250, in generate\n",
            "    result = self._sample(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 3238, in _sample\n",
            "    outputs = self(**model_inputs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 855, in forward\n",
            "    outputs = self.model(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 579, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 276, in forward\n",
            "    hidden_states = self.mlp(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 57, in forward\n",
            "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py\", line 432, in forward\n",
            "    return F.silu(input, inplace=self.inplace)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/functional.py\", line 2380, in silu\n",
            "    return torch._C._nn.silu(input)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 8 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 9 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1874, in __getattr__\n",
            "    value = getattr(module, name)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1873, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1885, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/auto/modeling_auto.py\", line 21, in <module>\n",
            "    from .auto_factory import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 40, in <module>\n",
            "    from ...generation import GenerationMixin\n",
            "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1873, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1885, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 29, in <module>\n",
            "    from transformers.generation.candidate_generator import AssistantVocabTranslatorCache\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/candidate_generator.py\", line 30, in <module>\n",
            "    from ..pytorch_utils import isin_mps_friendly\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/pytorch_utils.py\", line 49, in <module>\n",
            "    from torch.distributed.tensor.parallel import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/distributed/tensor/parallel/__init__.py\", line 2, in <module>\n",
            "    from torch.distributed.tensor.parallel.api import parallelize_module\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/distributed/tensor/parallel/api.py\", line 9, in <module>\n",
            "    from torch.distributed.tensor.parallel._utils import _validate_tp_mesh_dim\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/distributed/tensor/parallel/_utils.py\", line 11, in <module>\n",
            "    from torch._dynamo.external_utils import is_compiling as is_torchdynamo_compiling\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_dynamo/__init__.py\", line 3, in <module>\n",
            "    from . import convert_frame, eval_frame, resume_execution\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 33, in <module>\n",
            "    from torch._dynamo.symbolic_convert import TensorifyState\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 30, in <module>\n",
            "    from . import config, exc, logging as torchdynamo_logging, trace_rules, variables\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_dynamo/trace_rules.py\", line 46, in <module>\n",
            "    from .variables import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/__init__.py\", line 2, in <module>\n",
            "    from .builtin import BuiltinVariable\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/builtin.py\", line 70, in <module>\n",
            "    from .tensor import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_dynamo/variables/tensor.py\", line 30, in <module>\n",
            "    from .._trace_wrapped_higher_order_op import trace_wrapped\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_dynamo/_trace_wrapped_higher_order_op.py\", line 55, in <module>\n",
            "    @torch.library.custom_op(\"flex_lib::zeros_and_scatter\", mutates_args=())  # type: ignore[misc]\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py\", line 126, in inner\n",
            "    result = CustomOpDef(namespace, opname, schema_str, fn)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py\", line 174, in __init__\n",
            "    self._register_to_dispatcher()\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py\", line 594, in _register_to_dispatcher\n",
            "    lib._register_fake(self._name, fake_impl, _stacklevel=4)\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/library.py\", line 177, in _register_fake\n",
            "    source = torch._library.utils.get_source(_stacklevel + 1)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_library/utils.py\", line 53, in get_source\n",
            "    frame = inspect.getframeinfo(sys._getframe(stacklevel))\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/inspect.py\", line 1718, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "                  ^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/inspect.py\", line 1090, in findsource\n",
            "    module = getmodule(object, file)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/inspect.py\", line 1016, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 427, in realpath\n",
            "  File \"<frozen posixpath>\", line 471, in _joinrealpath\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 9 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 10 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2108, in <module>\n",
            "    from torch import _VF as _VF, functional as functional  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 8, in <module>\n",
            "    from torch.nn.modules import *  # usort: skip # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 2, in <module>\n",
            "    from .linear import Bilinear, Identity, LazyLinear, Linear  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py\", line 7, in <module>\n",
            "    from torch.nn import functional as F, init\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/functional.py\", line 11, in <module>\n",
            "    from torch._jit_internal import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_jit_internal.py\", line 43, in <module>\n",
            "    import torch.distributed.rpc\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/distributed/rpc/__init__.py\", line 28, in <module>\n",
            "    if is_available() and not torch._C._rpc_init():\n",
            "                              ^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 10 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 11 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 405, in <module>\n",
            "    from torch._C import *  # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 463, in _lock_unlock_module\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 11 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 12 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 405, in <module>\n",
            "    from torch._C import *  # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 463, in _lock_unlock_module\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 12 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 13 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2108, in <module>\n",
            "    from torch import _VF as _VF, functional as functional  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 8, in <module>\n",
            "    from torch.nn.modules import *  # usort: skip # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 1, in <module>\n",
            "    from .module import Module  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 29, in <module>\n",
            "    from torch.utils._python_dispatch import is_traceable_wrapper_subclass\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/_python_dispatch.py\", line 12, in <module>\n",
            "    import torchgen.model\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torchgen/model.py\", line 2170, in <module>\n",
            "    @dataclass(frozen=True)\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 1265, in wrap\n",
            "    return _process_class(cls, init, repr, eq, order, unsafe_hash,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 1063, in _process_class\n",
            "    _init_fn(all_init_fields,\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 619, in _init_fn\n",
            "    return _create_fn('__init__',\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 473, in _create_fn\n",
            "    exec(txt, globals, ns)\n",
            "  File \"<string>\", line 0, in <module>\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 13 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 14 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 21, in <module>\n",
            "    from huggingface_hub import get_full_repo_name  # for backward compatibility\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/huggingface_hub/__init__.py\", line 962, in __getattr__\n",
            "    submod = importlib.import_module(submod_path)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py\", line 63, in <module>\n",
            "    from ._inference_endpoints import InferenceEndpoint, InferenceEndpointType\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/huggingface_hub/_inference_endpoints.py\", line 9, in <module>\n",
            "    from .inference._client import InferenceClient\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py\", line 45, in <module>\n",
            "    from huggingface_hub.inference._common import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_common.py\", line 53, in <module>\n",
            "    from ._generated.types import ChatCompletionStreamOutput, TextGenerationStreamOutput\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/types/__init__.py\", line 77, in <module>\n",
            "    from .image_to_text import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/types/image_to_text.py\", line 14, in <module>\n",
            "    @dataclass_with_extra\n",
            "     ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/types/base.py\", line 36, in dataclass_with_extra\n",
            "    cls = dataclass(cls)\n",
            "          ^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 1275, in dataclass\n",
            "    return wrap(cls)\n",
            "           ^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 1265, in wrap\n",
            "    return _process_class(cls, init, repr, eq, order, unsafe_hash,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 1063, in _process_class\n",
            "    _init_fn(all_init_fields,\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 619, in _init_fn\n",
            "    return _create_fn('__init__',\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 461, in _create_fn\n",
            "    body = '\\n'.join(f'  {b}' for b in body)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 461, in <genexpr>\n",
            "    body = '\\n'.join(f'  {b}' for b in body)\n",
            "\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 14 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 15 ËΩÆÂÆûÈ™å...\n",
            "[INFO]: Running simulation run 1 out of 30\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id A1\n",
            " 11% 1/9 [00:02<00:19,  2.46s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 211, in <module>\n",
            "    main()\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 177, in main\n",
            "    tutor_history = simulate_conversation(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 100, in simulate_conversation\n",
            "    tutor_message = model.generate(tutor_history)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 92, in generate\n",
            "    output = self.model.generate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2250, in generate\n",
            "    result = self._sample(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 3238, in _sample\n",
            "    outputs = self(**model_inputs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 855, in forward\n",
            "    outputs = self.model(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 579, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 260, in forward\n",
            "    hidden_states, self_attn_weights = self.self_attn(\n",
            "                                       ^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 205, in forward\n",
            "    attn_output = self.o_proj(attn_output)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 15 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 16 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 405, in <module>\n",
            "    from torch._C import *  # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 463, in _lock_unlock_module\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 16 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 17 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2222, in <module>\n",
            "    from torch import quantization as quantization  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/quantization/__init__.py\", line 2, in <module>\n",
            "    from .fake_quantize import *  # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/quantization/fake_quantize.py\", line 10, in <module>\n",
            "    from torch.ao.quantization.fake_quantize import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/ao/quantization/__init__.py\", line 28, in <module>\n",
            "    from .quantization_mappings import *  # noqa: F403 # type: ignore[no-redef]\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/ao/quantization/quantization_mappings.py\", line 14, in <module>\n",
            "    import torch.ao.nn.quantized.reference as nnqr\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/ao/nn/quantized/reference/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/ao/nn/quantized/reference/modules/__init__.py\", line 1, in <module>\n",
            "    from .conv import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/ao/nn/quantized/reference/modules/conv.py\", line 273, in <module>\n",
            "    class ConvTranspose1d(_ConvTransposeNd, nn.ConvTranspose1d):\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/ao/nn/quantized/reference/modules/conv.py\", line 308, in ConvTranspose1d\n",
            "    self, x: torch.Tensor, output_size: Optional[List[int]] = None\n",
            "                                        ~~~~~~~~^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/typing.py\", line 395, in inner\n",
            "    return _caches[func](*args, **kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/typing.py\", line 517, in __getitem__\n",
            "    return self._getitem(self, parameters)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/typing.py\", line 751, in Optional\n",
            "    return Union[arg, type(None)]\n",
            "           ~~~~~^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/typing.py\", line 395, in inner\n",
            "    return _caches[func](*args, **kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/typing.py\", line 517, in __getitem__\n",
            "    return self._getitem(self, parameters)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/typing.py\", line 730, in Union\n",
            "    parameters = tuple(_type_check(p, msg) for p in parameters)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/typing.py\", line 730, in <genexpr>\n",
            "    parameters = tuple(_type_check(p, msg) for p in parameters)\n",
            "\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 17 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 18 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2108, in <module>\n",
            "    from torch import _VF as _VF, functional as functional  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 8, in <module>\n",
            "    from torch.nn.modules import *  # usort: skip # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 2, in <module>\n",
            "    from .linear import Bilinear, Identity, LazyLinear, Linear  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py\", line 7, in <module>\n",
            "    from torch.nn import functional as F, init\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/functional.py\", line 11, in <module>\n",
            "    from torch._jit_internal import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_jit_internal.py\", line 43, in <module>\n",
            "    import torch.distributed.rpc\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/distributed/rpc/__init__.py\", line 77, in <module>\n",
            "    from .server_process_global_profiler import _server_process_global_profile\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/distributed/rpc/server_process_global_profiler.py\", line 8, in <module>\n",
            "    from torch.autograd.profiler_legacy import profile\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 15, in <module>\n",
            "    from torch import _vmap_internals\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_vmap_internals.py\", line 8, in <module>\n",
            "    from torch.utils._pytree import _broadcast_to_and_flatten, tree_flatten, tree_unflatten\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/_pytree.py\", line 416, in <module>\n",
            "    @dataclasses.dataclass(frozen=True)\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 1265, in wrap\n",
            "    return _process_class(cls, init, repr, eq, order, unsafe_hash,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 1092, in _process_class\n",
            "    _cmp_fn('__eq__', '==',\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 667, in _cmp_fn\n",
            "    return _create_fn(name,\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 473, in _create_fn\n",
            "    exec(txt, globals, ns)\n",
            "  File \"<string>\", line 0, in <module>\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 18 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 19 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 405, in <module>\n",
            "    from torch._C import *  # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 19 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 20 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2108, in <module>\n",
            "    from torch import _VF as _VF, functional as functional  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 8, in <module>\n",
            "    from torch.nn.modules import *  # usort: skip # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 2, in <module>\n",
            "    from .linear import Bilinear, Identity, LazyLinear, Linear  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py\", line 7, in <module>\n",
            "    from torch.nn import functional as F, init\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/functional.py\", line 11, in <module>\n",
            "    from torch._jit_internal import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_jit_internal.py\", line 43, in <module>\n",
            "    import torch.distributed.rpc\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/distributed/rpc/__init__.py\", line 77, in <module>\n",
            "    from .server_process_global_profiler import _server_process_global_profile\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/distributed/rpc/server_process_global_profiler.py\", line 8, in <module>\n",
            "    from torch.autograd.profiler_legacy import profile\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 21, in <module>\n",
            "    from .function import Function, NestedIOFunction\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/autograd/function.py\", line 643, in <module>\n",
            "    class InplaceFunction(Function):\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/autograd/function.py\", line 329, in __init__\n",
            "    def __init__(cls, name, bases, attrs):\n",
            "\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 20 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 21 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2151, in <module>\n",
            "    from torch import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/profiler/__init__.py\", line 18, in <module>\n",
            "    from .profiler import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/profiler/profiler.py\", line 25, in <module>\n",
            "    from torch.profiler._memory_profiler import MemoryProfile, MemoryProfileTimeline\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1128, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 757, in _compile_bytecode\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 21 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 22 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2151, in <module>\n",
            "    from torch import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/multiprocessing/__init__.py\", line 41, in <module>\n",
            "    from .spawn import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 4, in <module>\n",
            "    import multiprocessing.connection\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1128, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 757, in _compile_bytecode\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 22 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 23 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2108, in <module>\n",
            "    from torch import _VF as _VF, functional as functional  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 8, in <module>\n",
            "    from torch.nn.modules import *  # usort: skip # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 1, in <module>\n",
            "    from .module import Module  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 29, in <module>\n",
            "    from torch.utils._python_dispatch import is_traceable_wrapper_subclass\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/__init__.py\", line 8, in <module>\n",
            "    from torch.utils import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/data/__init__.py\", line 1, in <module>\n",
            "    from torch.utils.data.dataloader import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 21, in <module>\n",
            "    import torch.utils.data.graph_settings\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/data/graph_settings.py\", line 8, in <module>\n",
            "    from torch.utils.data.datapipes.iter.sharding import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/data/datapipes/__init__.py\", line 1, in <module>\n",
            "    from torch.utils.data.datapipes import dataframe as dataframe, iter as iter, map as map\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/data/datapipes/iter/__init__.py\", line 27, in <module>\n",
            "    from torch.utils.data.datapipes.iter.routeddecoder import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/data/datapipes/iter/routeddecoder.py\", line 7, in <module>\n",
            "    from torch.utils.data.datapipes.utils.decoder import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/data/datapipes/utils/decoder.py\", line 144, in <module>\n",
            "    class ImageHandler:\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 23 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 24 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 405, in <module>\n",
            "    from torch._C import *  # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 463, in _lock_unlock_module\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 24 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 25 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1885, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 119, in <module>\n",
            "    from accelerate.hooks import AlignDevicesHook, add_hook_to_module\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/accelerate/__init__.py\", line 16, in <module>\n",
            "    from .accelerator import Accelerator\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/accelerate/accelerator.py\", line 36, in <module>\n",
            "    from accelerate.utils.imports import is_torchao_available\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/accelerate/utils/__init__.py\", line 216, in <module>\n",
            "    from .bnb import has_4bit_bnb_layers, load_and_quantize_model\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/accelerate/utils/bnb.py\", line 29, in <module>\n",
            "    from ..big_modeling import dispatch_model, init_empty_weights\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/accelerate/big_modeling.py\", line 24, in <module>\n",
            "    from .hooks import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/accelerate/hooks.py\", line 37, in <module>\n",
            "    from .utils.other import recursive_getattr\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/accelerate/utils/other.py\", line 233, in <module>\n",
            "    TORCH_SAFE_GLOBALS.append(np.dtypes.UInt32DType)\n",
            "                              ^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/numpy/__init__.py\", line 337, in __getattr__\n",
            "    import numpy.dtypes as dtypes\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/numpy/__init__.py\", line 337, in __getattr__\n",
            "    import numpy.dtypes as dtypes\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/numpy/__init__.py\", line 337, in __getattr__\n",
            "    import numpy.dtypes as dtypes\n",
            "  [Previous line repeated 913 more times]\n",
            "RecursionError: maximum recursion depth exceeded\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1885, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/auto/modeling_auto.py\", line 21, in <module>\n",
            "    from .auto_factory import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 40, in <module>\n",
            "    from ...generation import GenerationMixin\n",
            "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1873, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1887, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\n",
            "maximum recursion depth exceeded\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1874, in __getattr__\n",
            "    value = getattr(module, name)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1873, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1887, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.models.auto.modeling_auto because of the following error (look up to see its traceback):\n",
            "Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\n",
            "maximum recursion depth exceeded\n",
            "‚úÖ Á¨¨ 25 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 26 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2108, in <module>\n",
            "    from torch import _VF as _VF, functional as functional  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 8, in <module>\n",
            "    from torch.nn.modules import *  # usort: skip # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 77, in <module>\n",
            "    from .fold import Fold, Unfold\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1322, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 1256, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap>\", line 1226, in __exit__\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 26 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 27 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2604, in <module>\n",
            "    from torch import _meta_registrations\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_meta_registrations.py\", line 11, in <module>\n",
            "    from torch._decomp import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_decomp/__init__.py\", line 284, in <module>\n",
            "    import torch._decomp.decompositions\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_decomp/decompositions.py\", line 15, in <module>\n",
            "    import torch._prims as prims\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_prims/__init__.py\", line 757, in <module>\n",
            "    expm1 = _make_elementwise_unary_prim(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_prims/__init__.py\", line 492, in _make_elementwise_unary_prim\n",
            "    return _make_prim(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_prims/__init__.py\", line 320, in _make_prim\n",
            "    prim_def = torch.library.custom_op(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py\", line 145, in custom_op\n",
            "    return inner(fn)\n",
            "           ^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py\", line 126, in inner\n",
            "    result = CustomOpDef(namespace, opname, schema_str, fn)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py\", line 174, in __init__\n",
            "    self._register_to_dispatcher()\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py\", line 596, in _register_to_dispatcher\n",
            "    autograd_impl = autograd.make_autograd_impl(self._opoverload, self)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_library/autograd.py\", line 28, in make_autograd_impl\n",
            "    @dataclass\n",
            "     ^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 1275, in dataclass\n",
            "    return wrap(cls)\n",
            "           ^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 1265, in wrap\n",
            "    return _process_class(cls, init, repr, eq, order, unsafe_hash,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 1083, in _process_class\n",
            "    _set_new_attribute(cls, '__repr__', _repr_fn(flds, globals))\n",
            "  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/dataclasses.py\", line 870, in _set_new_attribute\n",
            "    setattr(cls, name, value)\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 27 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 28 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2151, in <module>\n",
            "    from torch import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/optim/__init__.py\", line 9, in <module>\n",
            "    from torch.optim import lr_scheduler as lr_scheduler, swa_utils as swa_utils\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/optim/swa_utils.py\", line 367, in <module>\n",
            "    class SWALR(LRScheduler):\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 28 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 29 ËΩÆÂÆûÈ™å...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2604, in <module>\n",
            "    from torch import _meta_registrations\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_meta_registrations.py\", line 11, in <module>\n",
            "    from torch._decomp import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_decomp/__init__.py\", line 284, in <module>\n",
            "    import torch._decomp.decompositions\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_decomp/decompositions.py\", line 15, in <module>\n",
            "    import torch._prims as prims\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_prims/__init__.py\", line 1029, in <module>\n",
            "    fmax = _make_elementwise_binary_prim(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_prims/__init__.py\", line 507, in _make_elementwise_binary_prim\n",
            "    return _make_prim(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_prims/__init__.py\", line 355, in _make_prim\n",
            "    if not any(contains_tensor_types(a.type) for a in _prim._schema.arguments) or str(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_prims/__init__.py\", line 355, in <genexpr>\n",
            "    if not any(contains_tensor_types(a.type) for a in _prim._schema.arguments) or str(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_subclasses/fake_impls.py\", line 108, in contains_tensor_types\n",
            "    tensor_type = torch._C.TensorType.get()\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 29 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®Á¨¨ 30 ËΩÆÂÆûÈ™å...\n",
            "[INFO]: Running simulation run 1 out of 30\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 211, in <module>\n",
            "    main()\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 148, in main\n",
            "    model = load_model_backend(\n",
            "            ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/interact_llm/utils/model_load.py\", line 109, in load_model_backend\n",
            "    model.load()\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 44, in load\n",
            "    self.model = AutoModelForCausalLM.from_pretrained(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 273, in _wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 4451, in from_pretrained\n",
            "    ) = cls._load_pretrained_model(\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 5060, in _load_pretrained_model\n",
            "    gc.collect()\n",
            "KeyboardInterrupt\n",
            "‚úÖ Á¨¨ 30 ËΩÆÂÆûÈ™åÂ∑≤ÂÆåÊàêÂπ∂‰øùÂ≠òÁªìÊûú„ÄÇ\n",
            "üéä ÊÅ≠ÂñúÔºÅ30 ËΩÆ‰∏≠ÊñáÂØπËØùÊ®°ÊãüÂÖ®ÈÉ®ËøêË°åÂÆåÊØïÔºÅ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4rXPqECEQvv",
        "outputId": "352dcfe0-6e3a-45d4-a4b3-55f01a6d1a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/ (stored 0%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-000903.json (deflated 53%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-002113.json (deflated 57%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-001059.json (deflated 57%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-010626.json (deflated 56%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-005620.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-002150.json (deflated 52%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260128-235356.json (deflated 53%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-005942.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-003355.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-010453.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-003303.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-003455.json (deflated 56%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-005259.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-004520.json (deflated 57%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-004100.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-005356.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-004655.json (deflated 59%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-001202.json (deflated 56%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-002631.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-005224.json (deflated 57%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-005137.json (deflated 58%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-001025.json (deflated 54%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-004615.json (deflated 60%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-001550.json (deflated 60%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-005444.json (deflated 56%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260128-235744.json (deflated 56%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-003606.json (deflated 57%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260128-235651.json (deflated 57%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-003824.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-001417.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-005757.json (deflated 54%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-001516.json (deflated 53%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-004934.json (deflated 56%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-001934.json (deflated 56%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-001637.json (deflated 56%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-003019.json (deflated 51%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-005509.json (deflated 57%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-003948.json (deflated 58%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260128-235923.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-000518.json (deflated 58%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-000802.json (deflated 56%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-010540.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-004741.json (deflated 56%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-000646.json (deflated 58%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-010803.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-000223.json (deflated 59%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-002713.json (deflated 57%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-002449.json (deflated 57%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-003653.json (deflated 54%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-005704.json (deflated 58%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-002038.json (deflated 54%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260128-235159.json (deflated 56%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-002903.json (deflated 56%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-005011.json (deflated 57%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-002208.json (deflated 59%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-010034.json (deflated 56%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260128-235836.json (deflated 52%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-002318.json (deflated 57%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-010221.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260128-235441.json (deflated 57%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-000125.json (deflated 55%)\n",
            "updating: content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1/20260129-003911.json (deflated 57%)\n",
            "‚úÖ ÊâìÂåÖÊàêÂäüÔºÅËØ∑ÂéªÂ∑¶‰æßÊñá‰ª∂Ê†èÊâæÂà∞ 'A1_data_backup.zip' Âπ∂‰∏ãËΩΩÂÆÉ„ÄÇ\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 1. ÂÆö‰πâ‰Ω†Ë¶ÅÊâìÂåÖÁöÑÊñá‰ª∂Â§πË∑ØÂæÑ (Ê†πÊçÆ‰Ω†ÁöÑÊó•ÂøóÊé®ÊµãÊòØËøô‰∏™)\n",
        "source_folder = \"/content/simulated_data/Qwen--Qwen2.5-1.5B-Instruct/v999.0/A1\"\n",
        "\n",
        "# 2. ÂÆö‰πâÂéãÁº©ÂåÖÁöÑÂêçÂ≠ó\n",
        "output_filename = \"/content/A1_data_backup.zip\"\n",
        "\n",
        "# 3. ÊâßË°åÂéãÁº©ÂëΩ‰ª§ (‰ΩøÁî®Á≥ªÁªüÂëΩ‰ª§ zipÔºåÈÄüÂ∫¶Âø´)\n",
        "# -r Ë°®Á§∫ÈÄíÂΩíÂéãÁº©Êï¥‰∏™Êñá‰ª∂Â§π\n",
        "!zip -r {output_filename} {source_folder}\n",
        "\n",
        "print(f\"‚úÖ ÊâìÂåÖÊàêÂäüÔºÅËØ∑ÂéªÂ∑¶‰æßÊñá‰ª∂Ê†èÊâæÂà∞ 'A1_data_backup.zip' Âπ∂‰∏ãËΩΩÂÆÉ„ÄÇ\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. ÊåÇËΩΩ‰∫ëÁ´ØÁ°¨Áõò (Â¶ÇÊûú‰πãÂâçÊåÇËΩΩËøáÔºåËøôÊ≠•‰ºöÂæàÂø´)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ÂÆö‰πâÊ∫êË∑ØÂæÑ (‰Ω†Áé∞Âú®‰øÆÊîπÂ•ΩÁöÑÈ°πÁõÆ)\n",
        "source_path = \"/content/Interact-LLM\"\n",
        "\n",
        "# ÂÆö‰πâÁõÆÊ†áË∑ØÂæÑ (‰Ω†ÁöÑ‰∫ëÁõòÈáåÔºåÊàëÁªôÂÆÉËµ∑ÂêçÂè´ Interact-LLM-Backup)\n",
        "destination_path = \"/content/drive/MyDrive/Interact-LLM-Backup\"\n",
        "\n",
        "print(\"üì¶ Ê≠£Âú®ÂºÄÂßãÂ§á‰ªΩÊï¥‰∏™È°πÁõÆÊñá‰ª∂Â§πÔºàÂåÖÂê´‰øÆÊîπËøáÁöÑ‰ª£Á†ÅÂíåÈÖçÁΩÆÔºâ...\")\n",
        "\n",
        "# 2. Â¶ÇÊûú‰∫ëÁõòÈáåÂ∑≤ÁªèÊúâÊóßÂ§á‰ªΩÔºåÂÖàÂà†ÊéâÔºå‰øùËØÅÂ≠òÂÖ•ÁöÑÊòØÊúÄÊñ∞Áâà\n",
        "if os.path.exists(destination_path):\n",
        "    shutil.rmtree(destination_path)\n",
        "\n",
        "# 3. Â§çÂà∂Êï¥‰∏™Êñá‰ª∂Â§π (ÂøΩÁï• .venv ËôöÊãüÁéØÂ¢ÉÂíå .git Êñá‰ª∂Â§π‰ª•ËäÇÁúÅÊó∂Èó¥ÔºåÂõ†‰∏∫‰∏ãÊ¨°ÁéØÂ¢ÉË¶ÅÈáçË£Ö)\n",
        "# Êàë‰ª¨Âè™Â§á‰ªΩ‰Ω†ÁöÑ‰ª£Á†Å„ÄÅÈÖçÁΩÆÂíå prompt Êñá‰ª∂\n",
        "def ignore_patterns(path, names):\n",
        "    return ['.venv', '.git', '__pycache__', 'results'] # results ‰Ω†Â∑≤ÁªèÂçïÁã¨ÊâìÂåÖ‰∏ãËΩΩ‰∫ÜÔºåËøôÈáåÂ∞±‰∏çÂ§á‰ªΩ‰∫ÜÔºåÁúÅÁ©∫Èó¥\n",
        "\n",
        "shutil.copytree(source_path, destination_path, ignore=ignore_patterns)\n",
        "\n",
        "print(f\"‚úÖ Â≠òÊ°£ÊàêÂäüÔºÅ\")\n",
        "print(f\"‰Ω†ÁöÑÈ°πÁõÆÂ∑≤‰øùÂ≠òÂú®: {destination_path}\")\n",
        "print(\"‰∏ãÊ¨°ÂõûÊù•Ôºå‰Ω†ÂÜç‰πü‰∏çÁî®ÈáçÊñ∞‰∏ä‰º† toml Êàñ‰øÆÊîπ detect_lang.py ‰∫ÜÔºÅ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wArDQOi3UUme",
        "outputId": "39390cdc-5ad6-4a29-bf33-0fd18b06f1d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "üì¶ Ê≠£Âú®ÂºÄÂßãÂ§á‰ªΩÊï¥‰∏™È°πÁõÆÊñá‰ª∂Â§πÔºàÂåÖÂê´‰øÆÊîπËøáÁöÑ‰ª£Á†ÅÂíåÈÖçÁΩÆÔºâ...\n",
            "‚úÖ Â≠òÊ°£ÊàêÂäüÔºÅ\n",
            "‰Ω†ÁöÑÈ°πÁõÆÂ∑≤‰øùÂ≠òÂú®: /content/drive/MyDrive/Interact-LLM-Backup\n",
            "‰∏ãÊ¨°ÂõûÊù•Ôºå‰Ω†ÂÜç‰πü‰∏çÁî®ÈáçÊñ∞‰∏ä‰º† toml Êàñ‰øÆÊîπ detect_lang.py ‰∫ÜÔºÅ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. ÊåÇËΩΩ\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. ÊÅ¢Â§çÈ°πÁõÆÂà∞ Colab\n",
        "source_backup = \"/content/drive/MyDrive/Interact-LLM-Backup\"\n",
        "work_dir = \"/content/Interact-LLM\"\n",
        "\n",
        "if not os.path.exists(work_dir):\n",
        "    print(\"üîÑ Ê≠£Âú®‰ªé‰∫ëÁõòÊÅ¢Â§ç‰Ω†ÁöÑÈ°πÁõÆ...\")\n",
        "    shutil.copytree(source_backup, work_dir)\n",
        "    print(\"‚úÖ È°πÁõÆÊñá‰ª∂Â∑≤ÊÅ¢Â§çÔºÅ‰Ω†ÁöÑ detect_lang.py Âíå toml ÈÖçÁΩÆÈÉΩÂú®„ÄÇ\")\n",
        "else:\n",
        "    print(\"È°πÁõÆÊñá‰ª∂Â§πÂ∑≤Â≠òÂú®„ÄÇ\")"
      ],
      "metadata": {
        "id": "JdcHMJIGUuBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a69ecd-be98-42bf-de60-dcd5a9445ff2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "üîÑ Ê≠£Âú®‰ªé‰∫ëÁõòÊÅ¢Â§ç‰Ω†ÁöÑÈ°πÁõÆ...\n",
            "‚úÖ È°πÁõÆÊñá‰ª∂Â∑≤ÊÅ¢Â§çÔºÅ‰Ω†ÁöÑ detect_lang.py Âíå toml ÈÖçÁΩÆÈÉΩÂú®„ÄÇ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Interact-LLM\n",
        "\n",
        "# ÂÆâË£Ö uv Âíå‰æùËµñÂåÖ (ËøôÊ≠•ÂæàÂø´)\n",
        "!pip install uv\n",
        "!uv sync\n",
        "\n",
        "print(\"‚úÖ ÁéØÂ¢ÉÂÆâË£ÖÂÆåÊØïÔºå‰Ω†ÂèØ‰ª•Áõ¥Êé•ÂºÄÂßãË∑ë B1 Âíå C1 ‰∫ÜÔºÅ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzbIipeHNuii",
        "outputId": "e0d9e864-1948-462b-c967-6bc18e178240"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Interact-LLM\n",
            "Collecting uv\n",
            "  Downloading uv-0.9.27-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.9.27-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.7/22.7 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uv\n",
            "Successfully installed uv-0.9.27\n",
            "Using CPython 3.12.12 interpreter at: \u001b[36m/usr/bin/python3\u001b[39m\n",
            "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
            "\u001b[2mResolved \u001b[1m63 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m62 packages\u001b[0m \u001b[2min 2m 04s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m62 packages\u001b[0m \u001b[2min 962ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.5.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.1.31\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.18.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.29.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1minteract-llm\u001b[0m\u001b[2m==0.1.0 (from file:///content/Interact-LLM)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlingua-language-detector\u001b[0m\u001b[2m==2.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlinkify-it-py\u001b[0m\u001b[2m==2.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmdit-py-plugins\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmlx\u001b[0m\u001b[2m==0.23.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmlx-lm\u001b[0m\u001b[2m==0.21.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.2.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.21.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==24.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.3.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.30.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.10.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.27.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2024.11.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==13.9.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mruff\u001b[0m\u001b[2m==0.11.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.5.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msentencepiece\u001b[0m\u001b[2m==0.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==76.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtextual\u001b[0m\u001b[2m==2.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtoml\u001b[0m\u001b[2m==0.10.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.50.0.dev0 (from git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.12.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1muc-micro-py\u001b[0m\u001b[2m==1.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
            "‚úÖ ÁéØÂ¢ÉÂÆâË£ÖÂÆåÊØïÔºå‰Ω†ÂèØ‰ª•Áõ¥Êé•ÂºÄÂßãË∑ë B1 Âíå C1 ‰∫ÜÔºÅ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Interact-LLM\n",
        "\n",
        "# ÊòéÁ°ÆÊåáÂÆöË∑ë B1 Á≠âÁ∫ßÔºåËÑöÊú¨‰ºöËá™Âä®Âæ™ÁéØ 30 Ê¨°\n",
        "print(\"üöÄ ÂºÄÂßãËøêË°å B1 Á≠âÁ∫ßÊ®°Êãü (ÂÖ± 30 ËΩÆ)...\")\n",
        "!NO_COLOR=1 uv run python src/scripts/alignment_drift/simulate.py \\\n",
        "    --model_name \"qwen25_15b\" \\\n",
        "    --prompt_version \"999\" \\\n",
        "    --prompt_id \"B1\" \\\n",
        "    --backend hf\n",
        "\n",
        "# Á≠â B1 ÂΩªÂ∫ïË∑ëÂÆåÂêéÔºåËá™Âä®ÂºÄÂßãË∑ë C1\n",
        "print(\"üöÄ B1 Â∑≤ÂÆåÊàêÔºåÂºÄÂßãËøêË°å C1 Á≠âÁ∫ßÊ®°Êãü (ÂÖ± 30 ËΩÆ)...\")\n",
        "!NO_COLOR=1 uv run python src/scripts/alignment_drift/simulate.py \\\n",
        "    --model_name \"qwen25_15b\" \\\n",
        "    --prompt_version \"999\" \\\n",
        "    --prompt_id \"C1\" \\\n",
        "    --backend hf\n",
        "\n",
        "print(\"üéä ÊâÄÊúâÁ≠âÁ∫ßÔºàB1, C1ÔºâÂ∑≤ÂÖ®ÈÉ®ÂÆåÊàêÔºÅ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj8pI6PQOG5L",
        "outputId": "58f7f3a7-ce54-48c8-d9ea-b6b12b3a45ed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Interact-LLM\n",
            "üöÄ ÂºÄÂßãËøêË°å B1 Á≠âÁ∫ßÊ®°Êãü (ÂÖ± 30 ËΩÆ)...\n",
            "[INFO]: Running simulation run 1 out of 30\n",
            "tokenizer_config.json: 7.30kB [00:00, 13.4MB/s]\n",
            "vocab.json: 2.78MB [00:00, 57.4MB/s]\n",
            "merges.txt: 1.67MB [00:00, 86.1MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 106MB/s]\n",
            "config.json: 100% 660/660 [00:00<00:00, 2.01MB/s]\n",
            "model.safetensors: 100% 3.09G/3.09G [03:04<00:00, 16.8MB/s]\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 962kB/s]\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id B1\n",
            "100% 9/9 [1:02:46<00:00, 418.50s/it]\n",
            "[INFO]: Running simulation run 2 out of 30\n",
            "Model qwen25_15b loaded successfully using hf backend (model_id = Qwen/Qwen2.5-1.5B-Instruct)\n",
            "[INFO]: Formatting prompts using toml file version 999.0 and prompt id B1\n",
            "  0% 0/9 [06:17<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 211, in <module>\n",
            "    main()\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 177, in main\n",
            "    tutor_history = simulate_conversation(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 117, in simulate_conversation\n",
            "    student_message = model.generate(student_history)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 92, in generate\n",
            "    output = self.model.generate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2250, in generate\n",
            "    result = self._sample(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 3241, in _sample\n",
            "    outputs = model_forward(**model_inputs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 855, in forward\n",
            "    outputs = self.model(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 579, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 276, in forward\n",
            "    hidden_states = self.mlp(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 57, in forward\n",
            "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "                                                                ^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "üöÄ B1 Â∑≤ÂÆåÊàêÔºåÂºÄÂßãËøêË°å C1 Á≠âÁ∫ßÊ®°Êãü (ÂÖ± 30 ËΩÆ)...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Interact-LLM/src/scripts/alignment_drift/simulate.py\", line 14, in <module>\n",
            "    from interact_llm.llm.hf_wrapper import ChatHF\n",
            "  File \"/content/Interact-LLM/src/interact_llm/llm/hf_wrapper.py\", line 8, in <module>\n",
            "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2604, in <module>\n",
            "    from torch import _meta_registrations\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_meta_registrations.py\", line 11, in <module>\n",
            "    from torch._decomp import (\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_decomp/__init__.py\", line 285, in <module>\n",
            "    import torch._refs\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_refs/__init__.py\", line 2553, in <module>\n",
            "    @register_decomposition(aten.std_mean)\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_decomp/__init__.py\", line 226, in decomposition_decorator\n",
            "    pytree.tree_map_(register, aten_op)\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/utils/_pytree.py\", line 1024, in tree_map_\n",
            "    deque(map(func, *flat_args), maxlen=0)  # consume and exhaust the iterable\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_decomp/__init__.py\", line 223, in register\n",
            "    _add_op_to_registry(registry, op, fn)\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_decomp/__init__.py\", line 91, in _add_op_to_registry\n",
            "    overloads.append(getattr(op, ol))\n",
            "                     ^^^^^^^^^^^^^^^\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_ops.py\", line 1094, in __getattr__\n",
            "    OpOverload(self, op_, op_dk_, schema, tags)\n",
            "  File \"/content/Interact-LLM/.venv/lib/python3.12/site-packages/torch/_ops.py\", line 686, in __init__\n",
            "    continue\n",
            "KeyboardInterrupt\n",
            "üéä ÊâÄÊúâÁ≠âÁ∫ßÔºàB1, C1ÔºâÂ∑≤ÂÖ®ÈÉ®ÂÆåÊàêÔºÅ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gvteZnDAO5x2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNNxO8j+ZA7c10v1NKsPNCO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}